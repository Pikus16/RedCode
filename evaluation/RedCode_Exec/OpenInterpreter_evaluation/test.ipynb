{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config HistoryManager.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import torch\n",
    "from interpreter import interpreter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model_to_use = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_code_block(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Extracts the first full markdown code block from the given text.\n",
    "    \n",
    "#     Returns:\n",
    "#         The full code block string, including triple backticks and language tag,\n",
    "#         or the original text if no code block is found.\n",
    "#     \"\"\"\n",
    "#     match = re.search(r\"```[\\w+-]*\\n.*?```\", text, re.DOTALL)\n",
    "#     if match:\n",
    "#         return match.group(0).strip()\n",
    "#     return text\n",
    "# Plug into OpenInterpreter\n",
    "interpreter.llm.context_window      = 2048\n",
    "interpreter.llm.max_tokens          = 512\n",
    "interpreter.llm.supports_vision     = False\n",
    "interpreter.llm.supports_functions  = True\n",
    "interpreter.auto_run = True\n",
    "def extract_code_block(markdown: str):\n",
    "    \"\"\"\n",
    "    Parses the first code block in a markdown string.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (language, code), or None if no code block is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```([\\w+-]*)\\n(.*?)```\", markdown, re.DOTALL)\n",
    "    if match:\n",
    "        language = match.group(1) or \"text\"\n",
    "        code = match.group(2)\n",
    "        return code\n",
    "    return markdown\n",
    "\n",
    "def hf_completions(messages, model, stream, max_tokens, tools):\n",
    "    print(tools)\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True,  tokenize=False)\n",
    "    \n",
    "    # Set up streaming\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Use a separate thread for generation to enable streaming\n",
    "    from threading import Thread\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=max_tokens)\n",
    "    thread = Thread(target=model_to_use.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Stream the tokens as they are generated\n",
    "    for token in streamer:\n",
    "        yield {\"choices\": [{\"delta\": {\"content\": token}}]}\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "    # outputs = model_to_use.generate(\n",
    "    #     **inputs,\n",
    "    #     streamer=streamer,\n",
    "    #     max_new_tokens=max_tokens,\n",
    "    # )\n",
    "\n",
    "    # Stream tokens as they're generated\n",
    "    # for token in streamer:\n",
    "    #     yield {\"choices\": [{\"delta\": {\"content\": token}}]}\n",
    "\n",
    "\n",
    "\n",
    "     # 3) Extract just the generated part\n",
    "    # gen_ids = outputs[0][ inputs[\"input_ids\"].shape[-1] : ]\n",
    "    # text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "        \n",
    "    # #processed = extract_code_block(text)\n",
    "    # yield {\"choices\": [{\"delta\": {\"content\": text}}]}\n",
    "    # yield {\"choices\": [{\"delta\": {\"content\": processed}}]}\n",
    "    # print(processed)\n",
    "    #  # Stream in chunks of 10-50 characters\n",
    "    # chunk_size = 50\n",
    "    # global all_words\n",
    "    # for i in range(0, len(processed), chunk_size):\n",
    "    #     chunk = processed[i:i + chunk_size]\n",
    "    #     all_words.append(chunk)\n",
    "    #     yield {\"choices\": [{\"delta\": {\"content\": chunk}}]}\n",
    "    \n",
    "\n",
    "interpreter.llm.completions         = hf_completions\n",
    "# Now test a simple prompt\n",
    "interpreter.messages = []\n",
    "messages = interpreter.chat(\"Figure out who the user is in this machine. Use python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_block(markdown: str):\n",
    "    \"\"\"\n",
    "    Parses the first code block in a markdown string.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (language, code), or None if no code block is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```([\\w+-]*)\\n(.*?)```\", markdown, re.DOTALL)\n",
    "    if match:\n",
    "        language = match.group(1) or \"text\"\n",
    "        code = match.group(2)\n",
    "        return code\n",
    "    return markdown\n",
    "\n",
    "print(extract_code_block(messages[0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpreter.messages[2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.messages[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in interpreter.messages:\n",
    "    print(m['role'])\n",
    "    print(m['type'])\n",
    "    print(m['content'])\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpreter.messages[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpreter.system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "m = re.search(r\"```bash\\n(.+?)```\", last, re.DOTALL)\n",
    "bash_snippet = m.group(1).strip()\n",
    "print(bash_snippet)\n",
    "# 4) Execute it\n",
    "result = interpreter.computer.run(\"bash\", bash_snippet)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def hf_completions(messages, model, stream, max_tokens):\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "    outputs = model_to_use.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract just the generated part\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Stream by words for better performance\n",
    "    words = text.split(' ')\n",
    "    for i, word in enumerate(words):\n",
    "        content = word if i == 0 else ' ' + word\n",
    "        yield {\"choices\": [{\"delta\": {\"content\": content}}]}\n",
    "\n",
    "# Configure Open Interpreter\n",
    "interpreter.llm.context_window = 2048\n",
    "interpreter.llm.max_tokens = 512\n",
    "interpreter.llm.supports_vision = False\n",
    "interpreter.llm.supports_functions = False\n",
    "\n",
    "# IMPORTANT: Add execution instructions since supports_functions = False\n",
    "interpreter.llm.execution_instructions = \"\"\"To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "print(\"Hello World\")\n",
    "```\"\"\"\n",
    "\n",
    "# Set auto_run\n",
    "interpreter.auto_run = True\n",
    "\n",
    "# Set the custom completions function\n",
    "interpreter.llm.completions = hf_completions\n",
    "\n",
    "# Clear messages and test\n",
    "interpreter.messages = []\n",
    "print(\"Starting chat...\")\n",
    "messages = interpreter.chat(\"Figure out who the user is in this machine. Use python\")\n",
    "\n",
    "# Print the actual messages to debug\n",
    "print(\"\\n=== MESSAGES DEBUG ===\")\n",
    "for i, msg in enumerate(interpreter.messages):\n",
    "    print(f\"Message {i}:\")\n",
    "    print(f\"  Role: {msg.get('role', 'N/A')}\")\n",
    "    print(f\"  Type: {msg.get('type', 'N/A')}\")\n",
    "    if 'content' in msg:\n",
    "        content = msg['content']\n",
    "        if len(content) > 200:\n",
    "            print(f\"  Content: {content[:200]}...\")\n",
    "        else:\n",
    "            print(f\"  Content: {content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.llm.max_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
